import argparse
import os

from pyspark.sql import DataFrame
import pyspark.sql.functions as F

from utils import create_spark_session, initialize_logger

# Initialize the logger
logger = initialize_logger("Data Analysis")


def tips_by_dropoff_zone(data: DataFrame) -> DataFrame:
    """
    Calculates tipping statistics by dropoff zone, including average tip amount,
    total number of tippers, and the proportion of trips that included a tip.

    Filters out zones with fewer than 10 trips to ensure statistical significance.

    Args:
        data (DataFrame): The input DataFrame containing trip data.

    Returns:
        DataFrame: A DataFrame with tipping statistics aggregated by dropoff zone.
    """
    result = (
        data
        # Create a new column 'has_tipped' indicating whether a tip was given
        .withColumn(
            "has_tipped",
            F.when(
                (F.col("tip_amount").isNotNull()) & (F.col("tip_amount") > 0),
                F.lit(1)
            )
            .otherwise(F.lit(0))
        )
        # Group data by dropoff location details and aggregate
        .groupby("dropoff_service_zone", "dropoff_borough", "dropoff_zone")
        .agg(
            F.avg(F.col("tip_amount")).alias("avg_tip"),
            F.sum(F.col("has_tipped")).alias("total_tippers"),
            F.count(F.lit(1)).alias("num_rows")
        )
        .filter(F.col("num_rows") > 10)
        .withColumn("proportion_tipped", F.col("total_tippers") / F.col("num_rows"))
        .sort(F.col("proportion_tipped").desc())
    )

    return result


def average_fare_by_destination(data: DataFrame) -> DataFrame:
    """
    Computes the average fare amount for each dropoff destination.

    Args:
        data (DataFrame): The input DataFrame containing trip data.

    Returns:
        DataFrame: A DataFrame with average fare amounts aggregated by dropoff destination.
    """
    result = (
        data
        # Group data by dropoff location details and aggregate statistics
        .groupby("dropoff_service_zone", "dropoff_borough", "dropoff_zone")
        .agg(
            F.avg(F.col("total_amount")).alias("avg_fare"),
            F.count(F.lit(1)).alias("num_trips")
        )
        .sort(F.col("avg_fare").desc())
    )

    return result


def popular_origin_destination(data: DataFrame) -> DataFrame:
    """
    Identifies the most popular origin-destination pairs based on the number of trips.
    Also calculates average fare, tip, and trip distance for each pair.

    Args:
        data (DataFrame): The input DataFrame containing trip data.

    Returns:
        DataFrame: A DataFrame listing origin-destination pairs with their statistics.
    """
    result = (
        data
        # Group data by pickup and dropoff zones and aggregate statistics
        .groupby("pickup_zone", "dropoff_zone")
        .agg(
            F.avg(F.col("total_amount")).alias("avg_fare"),
            F.avg(F.col("tip_amount")).alias("avg_tip"),
            F.avg(F.col("trip_distance")).alias("avg_distance"),
            F.count(F.lit(1)).alias("num_trips")
        )
        .sort(F.col("num_trips").desc())
    )

    return result


def parse_job_arguments() -> dict[str, str]:
    """
    Parses command-line arguments required for the data analysis job.

    Returns:
        dict[str, str]: A dictionary containing the parsed arguments.
    """
    arg_parser = argparse.ArgumentParser(description="Data Analysis Process Arguments")

    arg_parser.add_argument(
        "--source_data_path", required=True, type=str, help="Path of source data"
    )
    arg_parser.add_argument(
        "--tips_stats_path", required=True, type=str,
        help="Path to write tips statistics by destination data"
    )
    arg_parser.add_argument(
        "--avg_fare_path", required=True, type=str, help="Path to write average fare data"
    )
    arg_parser.add_argument(
        "--popular_origin_destination_path", required=True, type=str,
        help="Path to write popular origin-destination data"
    )

    job_step_args = arg_parser.parse_args()

    job_args = {
        "source_path": job_step_args.source_data_path,
        "tips_stats_path": job_step_args.tips_stats_path,
        "avg_fare_path": job_step_args.avg_fare_path,
        "popular_origin_destination_path": job_step_args.popular_origin_destination_path
    }

    return job_args


if __name__ == '__main__':
    # Parse arguments
    step_args = parse_job_arguments()

    # Determine if the script is running in a local environment
    IS_LOCAL = os.getenv("LOCAL").lower() == "true" if os.getenv("LOCAL") else False

    # Create a Spark session for the data analysis process
    spark = create_spark_session(app_name="Data Aggregation", is_local=IS_LOCAL, logger=logger)

    # Read source data generated by the previous step
    source_data = spark.read.parquet(step_args['source_path'] + "/*.parquet")
    logger.info(f"Data count: {source_data.count()}")  # Log the number of records loaded

    # Generate tipping statistics by dropoff zone, and output the results to S3
    tips_stats = tips_by_dropoff_zone(data=source_data)
    logger.info(f"Writing tips stats data to {step_args['tips_stats_path']}")
    tips_stats.write.mode("overwrite").parquet(step_args['tips_stats_path'])

    # Calculate the average fare by destination, and output the results to S3
    avg_fare_by_destination = average_fare_by_destination(data=source_data)
    logger.info(f"Writing average fare by destination stats data to {step_args['avg_fare_path']}")
    avg_fare_by_destination.write.mode("overwrite").parquet(step_args['avg_fare_path'])

    # Identify the most popular origin-destination pairs, and output the results to S3
    popular_origin_destination = popular_origin_destination(data=source_data)
    logger.info(f"Writing popular origin destination data to {step_args['popular_origin_destination_path']}")
    popular_origin_destination.write.mode("overwrite").parquet(step_args['popular_origin_destination_path'])

    logger.info("Completed")
